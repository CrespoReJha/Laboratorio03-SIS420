{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Laboratorio 3\n",
        "Nombre: Jhamil Crespo Rejas\n",
        "\n",
        "Carrera: Ingenieria en Ciencias de la Computacion"
      ],
      "metadata": {
        "id": "CgLUWDbXX1Ri"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Carga de datos y de librerias\n"
      ],
      "metadata": {
        "id": "MJCCGlt8aa2W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conectamos con el drive para poder acceder a los archivos"
      ],
      "metadata": {
        "id": "XH30opA-ao_Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQSFDUfrQeQV",
        "outputId": "88849f91-b1e6-472d-b023-9263ab206653"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importamos las librerias necesarias para trabajar con los datos"
      ],
      "metadata": {
        "id": "MWku03ufaxNV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "20Q29kX8SxgJ"
      },
      "outputs": [],
      "source": [
        "# utilizado para la manipulación de directorios y rutas\n",
        "import os\n",
        "\n",
        "# Cálculo científico y vectorial para python\n",
        "import numpy as np\n",
        "\n",
        "# Libreria para graficos\n",
        "from matplotlib import pyplot\n",
        "\n",
        "# Modulo de optimizacion en scipy\n",
        "from scipy import optimize\n",
        "\n",
        "# modulo para cargar archivos en formato MATLAB\n",
        "# from scipy.io import loadmat\n",
        "\n",
        "# le dice a matplotlib que incruste gráficos en el cuaderno\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargamos el dataset. En este caso utilizamos un dataset que contiene informacion sobre los 784 pixeles de una foto de una letra en el lenguaje de señas. Por lo tanto el dataset contiene 784 caracteristicas (784 pixeles) y la Y tiene 25 clases que corresponden a las letras del alfabeto en ingles.\n",
        "\n"
      ],
      "metadata": {
        "id": "Ah5jwKlwbCON"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "hhRjL2ptSxgK"
      },
      "outputs": [],
      "source": [
        "# La entrada es de 785 elementos contando con x0\n",
        "input_layer_size  = 785\n",
        "\n",
        "# 25 etiquetas, de 0 a 24\n",
        "num_labels = 25\n",
        "\n",
        "#  datos de entrenamiento almacenados en los arreglos X, y\n",
        "data = np.loadtxt('/content/drive/MyDrive/sign_mnist_train(preparado).csv', delimiter=',')\n",
        "X = data[:, 1:]\n",
        "y = data[:, 0]\n",
        "\n",
        "m = y.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "aw2yVc8ESxgL",
        "outputId": "fa1b0181-d600-4d82-f4aa-a827e077b87f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[107. 118. 127. 134. 139. 143. 146. 150. 153. 156. 158. 160. 163. 165.\n",
            " 159. 166. 168. 170. 170. 171. 171. 171. 172. 171. 171. 170. 170. 169.\n",
            " 111. 121. 129. 135. 141. 144. 148. 151. 154. 157. 160. 163. 164. 170.\n",
            " 119. 152. 171. 171. 170. 171. 172. 172. 172. 172. 172. 171. 171. 170.\n",
            " 113. 123. 131. 137. 142. 145. 150. 152. 155. 158. 161. 163. 164. 172.\n",
            " 105. 142. 170. 171. 171. 171. 172. 172. 173. 173. 172. 171. 171. 171.\n",
            " 116. 125. 133. 139. 143. 146. 151. 153. 156. 159. 162. 163. 167. 167.\n",
            "  95. 144. 171. 172. 172. 172. 172. 172. 173. 173. 173. 172. 172. 171.\n",
            " 117. 126. 134. 140. 145. 149. 153. 156. 158. 161. 163. 164. 175. 156.\n",
            "  87. 154. 172. 173. 173. 173. 173. 173. 174. 174. 174. 173. 172. 172.\n",
            " 119. 128. 136. 142. 146. 150. 153. 156. 159. 163. 165. 164. 184. 148.\n",
            "  89. 164. 172. 174. 174. 174. 174. 175. 175. 174. 175. 174. 173. 173.\n",
            " 122. 130. 138. 143. 147. 150. 154. 158. 162. 165. 166. 172. 181. 128.\n",
            "  94. 170. 173. 175. 174. 175. 176. 177. 177. 177. 177. 175. 175. 174.\n",
            " 122. 132. 139. 145. 149. 152. 156. 160. 163. 165. 166. 181. 172. 103.\n",
            " 113. 175. 176. 178. 178. 179. 179. 179. 179. 178. 179. 177. 175. 174.\n",
            " 125. 134. 141. 147. 150. 153. 157. 161. 164. 167. 168. 184. 179. 116.\n",
            " 126. 165. 176. 179. 180. 180. 181. 180. 180. 180. 179. 178. 177. 176.\n",
            " 128. 135. 142. 148. 152. 154. 158. 162. 165. 168. 170. 187. 180. 156.\n",
            " 161. 124. 143. 179. 178. 178. 181. 182. 181. 180. 181. 180. 179. 179.\n",
            " 129. 136. 144. 150. 153. 155. 159. 163. 166. 169. 172. 187. 184. 153.\n",
            " 102. 117. 110. 175. 169. 154. 182. 183. 183. 182. 182. 181. 181. 179.\n",
            " 131. 138. 145. 150. 155. 157. 161. 165. 168. 174. 190. 189. 175. 146.\n",
            "  94.  97. 113. 151. 158. 129. 184. 184. 184. 184. 183. 183. 182. 180.\n",
            " 131. 139. 146. 151. 155. 159. 163. 167. 175. 182. 179. 171. 159. 114.\n",
            " 102.  89. 121. 136. 136.  96. 172. 186. 186. 185. 185. 184. 182. 181.\n",
            " 131. 140. 147. 154. 157. 160. 164. 179. 186. 191. 187. 180. 157. 100.\n",
            "  88.  84. 108. 111. 126.  90. 120. 186. 187. 187. 186. 185. 184. 182.\n",
            " 133. 141. 149. 155. 158. 160. 174. 201. 189. 165. 151. 143. 146. 120.\n",
            "  87.  78.  87.  76. 108.  98.  96. 181. 188. 187. 186. 186. 185. 183.\n",
            " 133. 141. 150. 156. 160. 161. 179. 197. 174. 135.  99.  72.  95. 134.\n",
            "  97.  72.  74.  68. 116. 105. 108. 187. 189. 187. 187. 186. 186. 185.\n",
            " 134. 143. 151. 156. 161. 163. 179. 194. 156. 110.  74.  42.  52. 139.\n",
            "  94.  67.  75.  75. 118. 106. 129. 189. 191. 190. 188. 188. 187. 186.\n",
            " 135. 144. 152. 158. 163. 163. 177. 193. 161. 122.  84.  43.  71. 134.\n",
            "  81.  57.  71.  88. 112.  98. 157. 193. 193. 192. 190. 190. 189. 188.\n",
            " 136. 144. 152. 158. 162. 163. 176. 192. 164. 128.  98.  62.  60. 100.\n",
            "  71.  76.  96. 101. 105.  95. 174. 195. 194. 194. 194. 193. 191. 190.\n",
            " 137. 145. 152. 159. 164. 165. 178. 191. 164. 135. 113.  82.  59.  87.\n",
            "  98. 111. 120. 108.  97. 108. 190. 196. 195. 195. 194. 193. 193. 192.\n",
            " 139. 146. 154. 160. 164. 165. 175. 186. 163. 139. 112.  85.  67. 102.\n",
            " 126. 133. 126. 105. 104. 176. 197. 198. 197. 196. 195. 195. 194. 193.\n",
            " 138. 147. 155. 161. 165. 167. 172. 186. 163. 137. 107.  87.  76. 106.\n",
            " 122. 125. 117.  96. 156. 199. 199. 200. 198. 196. 196. 195. 195. 194.\n",
            " 139. 148. 156. 163. 166. 168. 172. 180. 158. 131. 108.  99.  86. 108.\n",
            " 118. 116. 103. 107. 191. 202. 201. 200. 200. 200. 199. 197. 198. 196.\n",
            " 140. 149. 157. 164. 168. 167. 177. 178. 155. 131. 118. 105.  87. 100.\n",
            " 106. 100.  96. 164. 202. 202. 202. 202. 202. 201. 200. 199. 199. 198.\n",
            " 140. 150. 157. 165. 167. 170. 181. 175. 152. 130. 115.  98.  82.  85.\n",
            "  90.  99. 165. 202. 203. 204. 203. 203. 202. 202. 201. 201. 200. 200.\n",
            " 142. 150. 159. 165. 170. 191. 173. 157. 144. 119.  97.  84.  79.  79.\n",
            "  91. 172. 202. 203. 203. 205. 204. 204. 204. 203. 202. 202. 201. 200.\n",
            " 142. 151. 160. 165. 188. 190. 187. 150. 119. 109.  85.  79.  79.  78.\n",
            " 137. 203. 205. 206. 206. 207. 207. 206. 206. 204. 205. 204. 203. 202.\n",
            " 142. 151. 160. 172. 196. 188. 188. 190. 135.  96.  86.  77.  77.  79.\n",
            " 176. 205. 207. 207. 207. 207. 207. 207. 206. 206. 206. 204. 203. 202.]\n",
            "[ 3.  6.  2. ... 18. 17. 23.]\n",
            "(27455, 784)\n"
          ]
        }
      ],
      "source": [
        "print(X[0,:])\n",
        "print(y)\n",
        "print(X.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos la funcion para normalizar los elementos de la matriz X y luego se llama a la misma"
      ],
      "metadata": {
        "id": "lYkf-IYEk3bm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def  featureNormalize(X):\n",
        "    X_norm = X.copy() #Crea una copia de X\n",
        "    mu = np.zeros(X.shape[1]) #crea un vector de ceros para inicializar mu\n",
        "    sigma = np.zeros(X.shape[1]) #inicializa sigma con ceros\n",
        "\n",
        "    mu = np.mean(X, axis = 0) #calcula la media\n",
        "    sigma = np.std(X, axis = 0) #calcula la desviacion estandar\n",
        "    X_norm = (X - mu) / sigma #normaliza cada caracteristica en X\n",
        "\n",
        "    return X_norm, mu, sigma #devuelve X normalizada, la media y la desviacion estandar"
      ],
      "metadata": {
        "id": "HWF5C5Jt8wNb"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# llama a featureNormalize con los datos cargados\n",
        "X_norm, mu, sigma = featureNormalize(X)"
      ],
      "metadata": {
        "id": "f4W6d-ya1ABV"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = X_norm"
      ],
      "metadata": {
        "id": "r9e6MRrW1G22"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos la funcion sigmoid para transformar valores al rango (0, 1) para calcular probabilidades"
      ],
      "metadata": {
        "id": "3Qga0C-US30G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "0sB9Kyi8SxgN"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "  #devuelve la sigmoide de z\n",
        "    return 1.0 / (1.0 + np.exp(-z))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos la funcion que calcula el costo y el gradiente"
      ],
      "metadata": {
        "id": "RaDzP9yYlmHa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "w5S0OOswSxgN"
      },
      "outputs": [],
      "source": [
        "def lrCostFunction(theta, X, y, lambda_):\n",
        "    # Inicializa m con la longitud de y\n",
        "    m = y.size\n",
        "\n",
        "    # convierte las etiquetas a valores enteros si son boleanos\n",
        "    if y.dtype == bool:\n",
        "        y = y.astype(int)\n",
        "    #inicializa el costo y el gradiente\n",
        "    J = 0\n",
        "    grad = np.zeros(theta.shape)\n",
        "\n",
        "    h = sigmoid(X.dot(theta.T)) #calcula las hipotesis\n",
        "\n",
        "    temp = theta\n",
        "    temp[0] = 0\n",
        "\n",
        "    #calculamos la funcion de costo regularizada\n",
        "    J = (1 / m) * np.sum(-y.dot(np.log(h)) - (1 - y).dot(np.log(1 - h))) + (lambda_ / (2 * m)) * np.sum(np.square(temp))\n",
        "    #calcula el gradiente regularizado\n",
        "    grad = (1 / m) * (h - y).dot(X)\n",
        "    grad = grad + (lambda_ / m) * temp\n",
        "\n",
        "    return J, grad"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos la funcion oneVsAll que entrena un clasificador de regresion logistica para cada clase. Esta funcion nos devuelve all_theta donde cada fila correspondera a los parametros entrenados para una etiqueta especifica"
      ],
      "metadata": {
        "id": "MveWDqplmgjp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "V0rOw5qhSxgN"
      },
      "outputs": [],
      "source": [
        "def oneVsAll(X, y, num_labels, lambda_):\n",
        "    # inicializa m y n con la cantidad de filas y columnas de X\n",
        "    m, n = X.shape\n",
        "\n",
        "    all_theta = np.zeros((num_labels, n + 1)) #inicializa all_theta\n",
        "\n",
        "    # Agrega unos a la matriz X\n",
        "    X = np.concatenate([np.ones((m, 1)), X], axis=1)\n",
        "\n",
        "    for c in np.arange(num_labels): #para cada clase\n",
        "        initial_theta = np.zeros(n + 1)\n",
        "        options = {'maxiter': 50} #cantidad de iteraciones\n",
        "        res = optimize.minimize(lrCostFunction,\n",
        "                                initial_theta,\n",
        "                                (X, (y == c), lambda_),\n",
        "                                jac=True,\n",
        "                                method='CG',\n",
        "                                options=options)\n",
        "\n",
        "        all_theta[c] = res.x\n",
        "\n",
        "    return all_theta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "w6JbsLLMSxgO",
        "outputId": "c5f69af9-3322-4409-b026-73ca87414b3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-e89dc4ff2744>:17: RuntimeWarning: divide by zero encountered in log\n",
            "  J = (1 / m) * np.sum(-y.dot(np.log(h)) - (1 - y).dot(np.log(1 - h))) + (lambda_ / (2 * m)) * np.sum(np.square(temp))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(25, 785)\n"
          ]
        }
      ],
      "source": [
        "lambda_ = 0.1\n",
        "all_theta = oneVsAll(X, y, num_labels, lambda_)\n",
        "print(all_theta.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(all_theta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1EfKaiEgtcw",
        "outputId": "2e982e64-f0b8-4873-f0b9-f3b45ed13798"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-9.00121690e+00  2.62196581e-01  2.27050751e-01 ... -8.08027661e-02\n",
            "  -1.20205360e-01 -1.29628222e-01]\n",
            " [-6.67273326e+00  4.07031116e-02 -4.73873021e-02 ...  6.12349156e-02\n",
            "  -3.99274753e-03 -1.40734371e-01]\n",
            " [-6.73854100e+00  3.24884549e-02  3.16214097e-03 ...  7.28362841e-02\n",
            "   1.73345469e-01  1.88221756e-01]\n",
            " ...\n",
            " [-5.28905946e+00 -4.17530981e-02 -2.32893186e-02 ...  7.66779626e-02\n",
            "   6.03086934e-02 -6.49361161e-02]\n",
            " [-7.85408776e+00  1.74384700e-01  3.78136455e-01 ... -2.90356180e-01\n",
            "  -1.91407613e-01  6.12096282e-02]\n",
            " [-7.40225497e+00  9.77884545e-02  1.11294017e-01 ... -5.72647732e-02\n",
            "  -1.66077780e-01 -3.87970821e-02]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos la funcion que realiza las predicciones"
      ],
      "metadata": {
        "id": "9GyUWBh0n78k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "PjFoFe1bSxgO"
      },
      "outputs": [],
      "source": [
        "def predictOneVsAll(all_theta, X):\n",
        "    m = X.shape[0]; #cantidad de ejemplos\n",
        "    num_labels = all_theta.shape[0] #cantidad de clases\n",
        "\n",
        "    p = np.zeros(m)\n",
        "\n",
        "    # Adiciona unos a X\n",
        "    X = np.concatenate([np.ones((m, 1)), X], axis=1)#adiciona la columna de unos\n",
        "    p = np.argmax(sigmoid(X.dot(all_theta.T)), axis = 1) #Realiza las predicciones\n",
        "\n",
        "    return p"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Realizamos una prueba de prediccion\n"
      ],
      "metadata": {
        "id": "c_KF4c0go5qM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "mE7v5cglSxgO",
        "outputId": "1289b544-224c-4c9e-bf4e-4bebaa9d8fb4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision del conjuto de entrenamiento: 99.69%\n",
            "[18 10 16 22 20 16 17 13 13 19 18 21 16 23  3 23 24 18 22  1  1 12  2  6\n",
            "  2 12 23  2  3 13 11 15 20  2  4 10  0 19  5  7  0 11  6 21 18 19 21  7\n",
            "  0  0  4 14 15 10  5 19  2 13  7 16 12  8 15 11 21 14 10  6 18 11 16 14\n",
            " 16 18  4  2 16 14 23  0 21 16 10 14 17  1 15 18  4 16 12 19 18 12 18 10\n",
            "  1  5 21 18 10 10 22  4 19 11 12  1 10  5 20  0 20 11 23 14  0 23 10  7\n",
            " 24  5 15 20 20 19  6 20 19  3 24  4 12 10  8  8  6 21 13  2]\n",
            "[18. 10. 16. 22. 20. 16. 17. 13. 13. 19. 18. 21. 16. 23.  3. 23. 24. 18.\n",
            " 22.  1.  1. 12.  2.  6.  2. 12. 23.  2.  3. 13. 11. 15. 20.  2.  4. 10.\n",
            "  0. 19.  5.  7.  0. 11.  6. 21. 18. 19. 21.  7.  0.  0.  4. 14. 15. 10.\n",
            "  5. 19.  2. 13.  7. 16. 12.  8. 15. 11. 21. 14. 10.  6. 18. 11. 16. 14.\n",
            " 16. 18.  4.  2. 16. 14. 23.  0. 21. 16. 10. 14. 17.  1. 15. 18.  4. 16.\n",
            " 12. 19. 18. 12. 18. 10.  1.  5. 21. 18. 10. 10. 22.  4. 19. 11. 12.  1.\n",
            " 10.  5. 20.  0. 20. 11. 23. 14.  0. 23. 10.  7. 24.  5. 15. 20. 20. 19.\n",
            "  6. 20. 19.  3. 24.  4. 12. 10.  8.  8.  6. 21. 13.  2.]\n",
            "100.0\n"
          ]
        }
      ],
      "source": [
        "pred = predictOneVsAll(all_theta, X)\n",
        "print('Precision del conjuto de entrenamiento: {:.2f}%'.format(np.mean(pred == y) * 100))\n",
        "\n",
        "XPrueba = X[10:150, :].copy()\n",
        "XPrueba = np.concatenate([np.ones((140, 1)), XPrueba], axis=1)\n",
        "\n",
        "p = np.argmax(sigmoid(XPrueba.dot(all_theta.T)), axis = 1)\n",
        "print(p)\n",
        "\n",
        "print(y[10:150])\n",
        "print(np.mean(p == y[10:150])*100) #indica que tan parecidos son los elementos de la prediccion y de los que deberia salir"
      ]
    }
  ]
}